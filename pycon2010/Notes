TODO
====

Read diveintopython3.org 20 times.

String Basics
=============

The main tool Python gives us to process text is strings - immutable sequence
of characters. There are usually two kinds of strings: plain strings, which
contain 8-bit (ASCII) characters; and Unicode Strings, which contain Unicode
characters.

String values can be enclosed in either a single or Double quotes. The two
different kinds of quotes work the same way, but having both allows you to
include one kind of quotes inside of a string specified with the other kind of
quotes, without needing to escape them with the backslash character.


How does the \ character at the end signify continuation?
---------------------------------------------------------

If you want a breaks in the string, you can have a triple quoted string (either
single quote or double quote)

Strings are immutable, which means that no matter what operations you do on a
string, you will always produce a new string object, rather than mutating the
existing string.

In general, you can do anything to a string that you can do to any other
sequence, as long as it doesn't require changing the sequence, since the
strings are immutable.

What is the difference between string.split method and string.splitlines method? 
--------------------------------------------------------------------------------

Useful code:

list_of_lines = one_large.string.splitlines()

one_large_string = '\n'.join(list_of_lines)

Python's standard module string offers much of the same functionality that is
available from the string methods, packaged up as functions instead of methods.

string.maketrans

You can create a set of characters from a string.

list("python")
map(somefunc,"python")
map(lambda x:x,"python")
sets.Set("python")

The built-in function ord also accepts as its argument, a Unicode string of
length of one, in which case it returns a Unicode code value, up to 65535.
Which means that the encoding which is used is UTF-16. 

To make a Unicode string of length one from a numeric Unicode code value, use
the built-in function unichr.

How to reverse a string in Python?
----------------------------------

For sequences, the extended slicing with negative step can be used for
reversing. [::-1] There is reversed built-in function which can also be used
for reversing the words in a sentence.

But for reversing the characters in a string, if reversed is used, then a
''.join needs to be called.

The reversed returns an iterator, suitable for looping on or for passing to
some "accumulator" callable such as ''.join. It does not return a ready made
string.

builtins
--------
1. repr
2. ord
3. chr
4. unichr


What are the Different kind of sequences in python

lists, strings, unicode objects, tuples, array.arrays etc.

(21:16:01) ivazquez|laptop: str → .decode() → unicode
(21:16:04) Yhg1s: phoe6: no. a bytestring contains encoded data. a unicode
object contains unencoded data. you decode a bytestring into a unicode string.
You encode a unicode string into a bytestring.

You never encode a sequence of bytes. 

EAFP - Its easier to ask forgiveness than permission. (EAFP). try/except is the
key tool in enabling the EAFP style.


What is a predicate?
--------------------
A term you can see often in discussions about programming is predicate, it just
means a function (or other callable object) that returns True or False as its
result. A predicate is said to be satisfied when it returns True.


string.translate and string.maketrans methods
---------------------------------------------

string.translate(s, table[, deletechars])

   Delete all characters from *s* that are in *deletechars* (if
   present), and then translate the characters using *table*, which
   must be a 256-character string giving the translation for each
   character value, indexed by its ordinal.  If *table* is ``None``,
   then only the character deletion step is performed.

string.maketrans(from, to)

   Return a translation table suitable for passing to ``translate()``,
   that will map each character in *from* into the character at the
   same position in *to*; *from* and *to* must have the same length.

   Note: Don't use strings derived from ``lowercase`` and ``uppercase`` as
     arguments; in some locales, these don't have the same length.
     For case conversions, always use ``str.lower()`` and
     ``str.upper()``.


Templating String functions are there.
--------------------------------------


What does u'\u2020' mean?
-------------------------

>>> print ord(u'\u2020')
8224

u'' is a unicode string object.
\u2020 is the unicode code point.

Questions
=========
Q: Convert a Hexadecimal Strings ("FF","FFFF") to Decimal
A: int("FF",16) and int("FFFF",16)

Q: Represent 266 in Hexadecimal.
A: print '%X' % 255

Q: Where do you see RFC 3548 being utilized?
Hint: import base64

What is the difference between string, bytes and buffer?

Python 2.0 had strings and Unicode Strings.
Python 3.0 has bytes and strings.
There will never be implicit conversion between bytes and strings.

Coming to Datatypes:
Strings are sequence of unicode characters, e.g. an HTML Document.
Bytes and byte arrays e.g. an JPEG Document.

In Python 3, all strings are sequences of Unicode characters.
Bytes are Bytes; characters are abstraction.

Here is a link between strings and bytes.

bytes object have a decode() method that takes a character encoding and returns a string.
string object has a encode method that takes a a character encoding and returns a bytes object.


What is the difference between linefeed and a newline?
newline is composed of Linefeed character. 

Outline for Review
------------------

* Outline of the Tutorial and a Brief Overview of Standard Library. (30 minutes)

        - Recollecting certain common programing paradigms.
                - Review of Regular Expressions.
                - Constructing lists with list comprehension.
                - Handling Exceptions.
                - Generators.
        - Interesting Modules that easy our task.
                - Collections module.
                - itertools module.

* Lets start with Strings. (15 Minutes)
        - String Filters. Filtering a set of characters from a string.
        - translate method on the string and maketrans function.
        - Checking whether a string contains a set of characters.
        - Expanding and Compression Tabs.
        - Making Multiple Replacements in a String in a Single Pass.
        - Reading a Text File by Paragraphs.

* Files - We deal with them often. (15 Minutes)
        - File Handling Basics.
        - Reading a Single line from a file, given a line number (linecache module)
        - Using Random Access Input/Output.
        - Walking a Directory Tree
        - Finding files given a search path and a pattern.

* Date time Related (15 Minutes)
        - Efficient Strategies for handling date and time related tasks.
        - datetime module, time module.
        - Calculating time periods in a date range.

* Processing XMLs. (15 Minutes for next two sections)
        - Parsing XML using xml.etree module.
        - Other XML parsing modules and their differences.

* Dealing with Database stuff.
        - Serializing Data using pickle and cPickle Modules.

Break Timing

* Interesting Topic (5 Minutes)

	- Difference between sort method on the list and a sorted function. Why the builtin sort is super-fast (because it is timsort!)

* Process Handling. (20 Minutes)
        - Subprocess module. 
        - Capturing the Output and Error Streams from a Unix Shell Command.
        - Forking a Daemon Process on Unix.
        - Program Execution on Windows.
        - Writing a Task Schedular.
        - Monitoring a Directory in an asynchronous way using Twisted.

* Network Programming (15 Minutes)
        - Detecting Inactive Computers in a Network using the Socket Module.
        - Connecting to IRC and logging the messages.

* Web Programming (10 Minutes)
        - urllib module.
        - Parsing RSS Feeds

* Unit Tests (10 Minutes)
        - How to write Unittests for your Python programs using unittest module.

* Programming tasks.(10 Minutes)
        - Implementing Tuples with named items.
        - Performance Measurements using timeit module

* How to Convert Python 2 to Python 3. (20 Minutes)
        - 2to3 tool.
        - Unicode Strings and Bytes.
        - How to handle the Strings vs bytes issue. Will *your* application be affected? How to identify and how to resolve.


Questions
=========

Q: What is a facade design pattern?
A: facade is the french term for face. It is the Interface. Look at facade.py

Q: What is a closure?
A: closure is an inner function that refers to names (variables) that are local
to an "outer" function containing it.

Q: What is the difference between sets.Set or the builtin set type?

Q: What is a bound method?
A: Whenver a Python object supplies a method, you can get the method, already
bound to the object, by just accessing the method on the object.
L = ['a','b','c']
x = L.append

What is class bytearray?
byte == 8 bits.
array == sequence.
bytearray object can be constructed using integers; text string along with an
encoding; using another bytes or bytearray; or any other object implementing a
buffer API.

Python's support for Unicode text, wide character strings used in
internationalized applications, as well as binary data - strings that represent
absolute byte values.

What is binary data, do you really care about it?

Python 3.0 provides an alternative string type for binary data and supports
Unicode text in its normal string type (ASCII is treated as a simple type of
Unicode)

Python 2.6 provides an alternative string type for non-ASCII  Unicode text, and
supports both simple text and binary data in its normal string type.

In a nutshell:

:: 

        ||*Python 2.6*||     *Python 3.0*||
        ||str         ||     unicode     ||
        ||unicode     || bytes,bytearray ||

Unicode Text: support of text encodings to be different in 3.0; direct,
accessible and seamless.

Binary Data: Image or audio files or packed data processed with the struct
module - you will need to understand 3.0's new bytes object and 3.0's different
and sharper distinction between text, binary data and files.

sys.getdefaultencoding()

Python's String Type

Python 2.x

str - 8 bit string type as well binary data. ( I can understand binary data,
but 8 bit string type??, should it not be 7 bit string type)

unicode - for representing wide character Unicode text.
unicode - allows for the extra size of characters and has extra support for
encodings and decodings.

Python 3.x comes with 3 string object types, one for textual data and two for
binary data.

str - for representing Unicode text.
bytes - for representing Binary data.
bytearray - a mutable flavor of bytes type.

3.0 str type defined an immutable sequence of characters (not neccesarily
bytes), which may be either normal text such as ASCII or Multi byte UTF-8.  A
new type called bytes was introduced to support truly binary data.

In 2.x; the general string type filled this binary data role, because strings
were just a sequence of bytes. In 3.0, the bytes type is defined as an
immutable sequence of 8-bit integers representing absolute byte values.  A 3.0
bytes object really is a sequence of small integers, each of which is in the
range 0 through 255; indexing a bytes returns int, slicing one returns another
bytes and running list() on one returns a list of integers, not characters.

While they were at it, the Python developers also added bytearray type in 3.0,
a variant of bytes, which is mutable and also supports in-place changes. The
bytearray type supports the usual string operations that str and bytes do, but
has inplace change operations also.

::

        ||*Python 2.6*|| *Python 3.0*||
        ||str || str, bytes||
        ||unicode || str and bytearray||

Because str and bytes are sharply differentiated by the language, the net
effect is that you must decide whether your data is text or binary in nature
and use 'str' or 'bytes' objects to represent its content in your script
respectively.

Ultimately, the mode in which you open a file will dictate which type of object
your script will use to represent its contents.

 * bytes or binary mode files.
 * bytearray to update data without making copies of it in memory.
 * If you are processing something that is textual in nature, such as program
   output, HTML, internationalized text, and CSV or XML files, you probably
   want to use str or text mode files.


Unicode Notes
=============

* [http://www.joelonsoftware.com/articles/Unicode.html Understanding Unicode]

* Content-Type tag in HTML? and emails have ???? ????
* Popular web development tool PHP, had a complete ignorance of character
  encoding issues.  blithely ( carelessly) using 8 bits for characters.
* Unicode, character sets, encoding is not that hard
* All that stuff about, plain text = ascii = characters are 8 bits is not only
  wrong but horribly wrong.
* When Kernighan and Ritche had invented The C Programming Language, everything
  was simple.  EBCDIC was on its way out.  ASCII was used.

Trivia:

In ASCII when you press CNTL, you subtract 64 from the value of the next
character.  So BELL is ASCII 7, which is CNTL+G, (CNTL is 64) and G is 71.
Codes below 32 were called unprintable. The space was 32 and letter A was 65.
This could conveniently be stored in 7 bits.  Most computers in those days were
using 8 bit bytes, so not only you could store all the ASCII characters, you
had a whole bit to spare. 

* Because bytes have room for upto eight bits, lots of people got into
  thinking, "gosh, we can use codes 128-255 for our own purposes." :) 
* Eventually, this OEM free-for-all got codified in the ANSI standard.
* In the ANSI standard, everyone agreed for bottom 128 but not the upper limits.
* Asian alphabets have thousands of letters, which were never going to fit into 8 bits.
* This was actually solved by a messy system called DBCS, the "double byte
  character set" in which some letters were stored in one byte and others took
  two bytes.
* It was easy to move forward in a string, but it was impossible to move
  backwards in the string.  Programmers were encouraged not to use s++ or s--
  but instead rely on Windows' AnsiNext and AnsiPrev functions which knew how
  to deal with that mess.

Unicode
=======

* Unicode was a brave effort to create a single character set that included
  every reasonable writing system on the planet.  Some people are under the
  mis-conception that unicode is simply a 16-bit code where each character
  takes 16 bits and therefore there are 65,536 possible characters.

* This is not correct. This is the single most common myth about unicode.
  Unicode has a different way of thinking about characters, and you have to
  understand the Unicode way of thinking of things or nothing will make sense.

* Until now, we've assumed that a letter maps to some bits which you can store
  on disk or in memory.

	A -> 0100 0001

* In unicode, a letter maps to something called code point, which is still just
  a theoretical concept.
* How that code point is represented in memory or on disk is a whole another
  story.
* In Unicode, the letter A is a platonic ideal. It's just floating in heaven.
* Every platonic letter in every alphabet is assigned a magic number by the
  Unicode consortium which is written like this: U+0639
* This magic number is called code-point.  The U+ means "Unicode" and the
  numbers are hexadecimal. U+0639 is the arabic letter Ain (ع).
* The English letter A would be U+0041 (A). You can find them all using the
  charmap utility on Windows 2000/XP.
* There is no real limit on the number of letters that Unicode can define and
  in fact, they have gone beyond 65,536 so not every unicode letter can really
  be squeezed into two bytes. That was a myth anyways.

OK, so we have a string:

	Hello

which, in Unicode, corresponds to these five code-points:
U+0048 U+0065 U+006C U+006C U+006F 

  (Hello)

It was U- before 3.0 and then it became U+. If you look at the release notes of
Unicode 3.0, you might find the reason for the change.


* How do we store those numbers?  That is where encoding comes in.
* The earliest idea was, that to store the numbers in two bytes each:

	00 48 00 65 00 6C 00 6C 00 6F.

* ( That is where the idea that unicode was 2 bytes had originated).
* Why not it be stored like this:

	48 00 65 00 6C 00 6C 00 6F 00

* Well, it could be stored in that way too. Early implementors wanted to store
  the numbers in either big-endian or little-endian, in whichever way their
  particular CPU  was fastest at... 
* So, people came up with Byte Order Mark, where FEFF denoted Little Endian and
  FFFE denoted big endian.
* For a while, it seemed like that might be good enough, but programmers were
  complaining. "Look at all those zeros!", they said, since they were Americans
  and they were looking at English text which rarely used code points above
  U+OOFF.  People decided to ignore Unicode and things got worse.
* And thus was invented the brilliant concept of UTF-8. (Read Rob Pike's mail)
* UTF-8 was another system for storing your string of unicode code points,
  those magic U+ numbers, in memory using 8 bits.
* In UTF-8, every code point from 0-127 is stored in a single byte. Only code
  points 128 and above are stored using 2, 3, in fact upto 6 bytes.  This has
  the neat side-effect that English text looks exactly the same in UTF-8 as it
  did in ASCII, so Americans don't even notice anything wrong.  
* Specifically, Hello which was "0048, 0065, 006C, 006C and 006F" would simply
  be stored as 48,65,6C,6C and 6F.

So far, we have discussed three ways of storing unicode:

* Traditional two bytes method; UCS-2 or UTF-16 ( It is no longer used). And
  you might also have to figure out, if it is a high-endian UCS-2 or low-endian
  UCS-2.
*  using UTF-8 standard.
* Where the heck is the third??

There a bunch of other ways of encoding Unicode. There is something called
UTF-7, which is lot like UTF-8 but guarantees that the high bit will always be
zero.  It was for systems which can recognize only 7 bits. UCS-4 which stores
each code point in 4 bytes, which has a nice property that every single code
point can be stored in same number of bytes. But that is memory hungry.

* Now that we are thinking of 'Platonic ideals' in unicode code-points, those
   unicode code-points can be encoded in any old-school encoding scheme too.
   Those code points can be encoded in any encoding scheme; but with one catch;
   some of the letters might not show up. ( Aha, here is the interesting
   thing).

If there is no equivalent for the Unicode code point you are trying to
represent in, you usually get a little question mark: ?  or a box. 

There are hundreds of traditional encodings, which can only store some
code-points correctly and change all other code points into question marks.

Some popular encodings of the English text are, Windows 1252 and ISO-8859-1,
aka Latin-1 (also useful for any western european languages). But try to store
Russian, or Hebrew letters in those encodings and you will get a bunch of
question marks. UTF 7, UTF 8, UTF 16 and UTF 32 all have the nice property of
being able to store any code point correctly.

_The Single Most Important Fact About Encodings._ 

_It does not make sense to have a string without knowing what encoding it uses._
_There is no such thing as plain text._

If you have a string in memory, in a file, or in an email message, you have to
know what encoding it is in or you cannot interpret it or display to your users
correctly.

All the problems of ????, comes down to the fact that if you don't tell me
whether a particular string is encoded using UTF-8 or ASCII or ISO 8859-1
(Latin 1) or Western 1252 (Western European), you simply cannot display it
correctly or even figure it out where it actually ends.

There are over 100 encodings, and above code point 127, all the bets are off.

How do we preserve this information about what encoding a string uses?  Email,
Content-Type: text/plain; charset="UTF-8"

For a web page, the original idea was that the web server would return a
similar Content-Type http header along with the web page itself -- not in the
HTML itself, but as one of the response headers that are sent before the HTML
page.

Relying on webserver to send Content-Type was problematic, because many
different people could use the same web-server for different types of web
pages.  It would be convenient, if you could put the Content-Type of the HTML
file right in the HTML file itself, using some kind of a special tag.  All
encoding uses same character between 32 and 127, so you could get to the point
wherein you could read the <meta> header.

The RFC which explains UTF-8

::
        http://www.ietf.org/rfc/rfc3629.txt

        The most interesting part of the RFC, which is leading me to understand the
        system better is explained here:

           The table below summarizes the format of these different octet types.
           The letter x indicates bits available for encoding bits of the
           character number.

           Char. number range  |        UTF-8 octet sequence
              (hexadecimal)    |              (binary)
           --------------------+---------------------------------------------
           0000 0000-0000 007F | 0xxxxxxx
           0000 0080-0000 07FF | 110xxxxx 10xxxxxx
           0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx
           0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx

           Encoding a character to UTF-8 proceeds as follows:

           1.  Determine the number of octets required from the character number
               and the first column of the table above.  It is important to note
               that the rows of the table are mutually exclusive, i.e., there is
               only one valid way to encode a given character.

           2.  Prepare the high-order bits of the octets as per the second
               column of the table.

           3.  Fill in the bits marked x from the bits of the character number,
               expressed in binary.  Start by putting the lowest-order bit of
               the character number in the lowest-order position of the last
               octet of the sequence, then put the next higher-order bit of the
               character number in the next higher-order position of that octet,
               etc.  When the x bits of the last octet are filled in, move on to
               the next to last octet, then to the preceding one, etc. until all
               x bits are filled in.

           The definition of UTF-8 prohibits encoding character numbers between
           U+D800 and U+DFFF, which are reserved for use with the UTF-16
           encoding form (as surrogate pairs) and do not directly represent
           characters.  When encoding in UTF-8 from UTF-16 data, it is necessary
           to first decode the UTF-16 data to obtain character numbers, which
           are then encoded in UTF-8 as described above.  This contrasts with
           CESU-8 [CESU-8], which is a UTF-8-like encoding that is not meant for
           use on the Internet.  CESU-8 operates similarly to UTF-8 but encodes
           the UTF-16 code values (16-bit quantities) instead of the character
           number (code point).  This leads to different results for character
           numbers above 0xFFFF; the CESU-8 encoding of those characters is NOT
           valid UTF-8.

           Decoding a UTF-8 character proceeds as follows:

           1.  Initialize a binary number with all bits set to 0.  Up to 21 bits
               may be needed.

           2.  Determine which bits encode the character number from the number
               of octets in the sequence and the second column of the table
               above (the bits marked x).

           3.  Distribute the bits from the sequence to the binary number, first
               the lower-order bits from the last octet of the sequence and
               proceeding to the left until no x bits are left.  The binary
               number is now equal to the character number.

           Implementations of the decoding algorithm above MUST protect against
           decoding invalid sequences.  For instance, a naive implementation may
           decode the overlong UTF-8 sequence C0 80 into the character U+0000,
           or the surrogate pair ED A1 8C ED BE B4 into U+233B4.  Decoding
           invalid sequences may have security consequences or cause other
           problems.  See Security Considerations (Section 10) below.

        4.  Syntax of UTF-8 Byte Sequences

           For the convenience of implementors using ABNF, a definition of UTF-8
           in ABNF syntax is given here.

           A UTF-8 string is a sequence of octets representing a sequence of UCS
           characters.  An octet sequence is valid UTF-8 only if it matches the
           following syntax, which is derived from the rules for encoding UTF-8
           and is expressed in the ABNF of [RFC2234].

           UTF8-octets = *( UTF8-char )
           UTF8-char   = UTF8-1 / UTF8-2 / UTF8-3 / UTF8-4
           UTF8-1      = %x00-7F
           UTF8-2      = %xC2-DF UTF8-tail
           UTF8-3      = %xE0 %xA0-BF UTF8-tail / %xE1-EC 2( UTF8-tail ) /
                         %xED %x80-9F UTF8-tail / %xEE-EF 2( UTF8-tail )
           UTF8-4      = %xF0 %x90-BF 2( UTF8-tail ) / %xF1-F3 3( UTF8-tail ) /
                         %xF4 %x80-8F 2( UTF8-tail )
           UTF8-tail   = %x80-BF

           NOTE -- The authoritative definition of UTF-8 is in [UNICODE].  This
           grammar is believed to describe the same thing Unicode describes, but
           does not claim to be authoritative.  Implementors are urged to rely
           on the authoritative source, rather than on this ABNF.

The official name of the encoding is UTF-8, where UTF stands for UCS
Transformation Format 8.  Write it as UTF-8 only.

So there is no limit on the number of the characters that Unicode could define.
So, it has definiely exceeded beyond, 65536 characters.

Exercise 1:
Convert the following to Unicode:
1) "Hello, World"
2) à¤¨à¤®à¤¸à¥à¤à¤¾à¤° à¤¦à¥à¤¨à¤¿à¤¯à¤¾ 

Answer:
1)"Hello, World" is present in U0000 and 
U+0048 U+0065 U+006C U+006C U+006F U+002C U+0057 U+006F U+0072 U+006C U+0064

2) à¤¨à¤®à¤¸à¥à¤à¤¾à¤° à¤¦à¥à¤¨à¤¿à¤¯à¤¾
is the devnagari script that starts with U0900 
U+0928 U+092E U+0938 U+0942 U+0915 U+090 U+0930 U+0926 U+0941 U+0928 U+092F U+093F U+0965


Thats good. 
- Confusion as where to use which code point.
- This may be different for different languages.
- xterm did not support it at all.
- 

The above was just a bunch of code points. We have not said anything about how
to store them in memory or represent them in email messages yet.

Encodings

English meaning of encoding is is wrapping it in a cipher code.  The earlier
method was to store those codepoints which are 4 hexadecimal digits as 2 bytes.
1 hexa digit can be written in 4 bits, 2 hexa digits can be written in 8 bits
which is 1 byte and so 4 hexa digits can be written in 2 bytes.

Convert Unicode to Hexadecimals.
Excellent tutorial.
http://ln.hixie.ch/?start=1064324988&count=1

Typing Unicode and maths symbols on gnome-terminal

1) Hold CTRL+SHIFT + U + codepoint + SPACE
2) For e.g. CTRL+SHIFT+U+2201+SPACE will give Unicode Maths Symbol 

Unicode code point chart:
http://inamidst.com/stuff/unidata/


Links
=====
unicode code-point chart:
http://inamidst.com/stuff/unidata/

File Operations
---------------

A variant of 'r' that is sometimes precious is 'rU', which tells Python to read
the file in text mode with "universal newlines": mode 'rU' can read text files
independently of the line termination convention the files are using, be it the
Unix way, the Windows way or even the (Old) Mac way. (Mac OS X today uses Unix
for all intents and purposes, but releases of Mac OS 9 and earlier were
different).

Files have other writing related methods such as flush, to send any data being
buffered, and writelines, to write a sequence of strings in a single call.
However, write is by far the most commonly used method.

File reading is more common.
When read is called with an integer argument N, it reads and returns the next N
bytes (or all the remaining bytes if less than N bytes remain)

Other methods worth mentioning are seek and tell, which support random access
to files. These methods are normally used with binary files made up of
fixed-length records.

Some Notes on Idiomatic Python
------------------------------

for line in input:
	process(line)

StringIO objects are plug-and-play compatible with file objects, so scanner
takes its three lines of text from an in-memory string object, rather than a
true external file.

Everywhere in Python, object interfaces, rather than specific data types are
units of coupling.

Often the data you want to write is not in one bit string, but in a list (or
other sequence) of strings. In this case, you should use the writelines method
(which despite its name, is not limited to lines and works just as well with
binary data as well as text files).

Calling writelines is much faster than the alternatives of joining the strings
into one big string (e.g, with the ''.join) and then calling write or calling
write repeatedly in a loop.

Calling close is even more advisable when you are writing to a file than you
are reading from a file.

Never trust your intuition in this matter - instead, always benchmark and
measure.

Counting tasks are notoriously prone to off-by-one errors.

Having a function accept an optional argument, while providing the most likely
value for the argument as the default value, is among the simplest and handiest
ways to implment this modest and often worthwhile kind of generalization.


struct.unpack - the format string - l stands for long.

With ZipFile, the flag is not used the same way when opening a file, and rb is
not recognized. The r flag handles the inherently binary nature of all zip
files on all platforms.

Adapter Design Pattern - What to do when you have an X and you need a Y
instead.

        
What is Global Interpretor Lock?
================================

Global Interpretor lock is used to protect the Python Objects from being
modified by multiple threads at once. To keep multiple threads running, the
interpretor automatically releases and reaquires the lock at regular intervals.
It also does this around potentially slow or blocking low level operations,
such a file and network I/O.  This is used internally to ensure that only one
thread runs in the Python VM at a time. Python offers to switch amongst threads
only between bytecode instructions. Each bytecode instruction and all C
implemented function is atomic from Python program's point of view.

Java and C# use shared memory concurrency model, with locking provided by
monitors. Message passing concurrency model have been implemented on top of the
existing shared memory concurrency model.
Erlang is using message passing concurrency model.
Alice extension to Standard ML, supports concurrency via Futures.
Cilk is concurrent C. There are developers at Akamai.

Process (computing)

Inter-Process Communication.

Some Historical Notes

By the early 60s computer control software had evolved from Monitor control
software, e.g., IBSYS, to Executive control software. Computers got "faster"
and computer time was still neither "cheap" nor fully used. It made
multiprogramming possible and necessary.

Multiprogramming means that several programs run "at the same time"
(concurrently). At first they ran on a single processor (i.e., uniprocessor)
and shared scarce resources. Multiprogramming is also basic form of
multiprocessing, a much broader term.

Programs consist of sequence of instruction for processor. Single processor can
run only one instruction at a time. Therefore it is impossible to run more
programs at the same time. Program might need some resource (input ...) which
has "big" delay. Program might start some slow operation (output to printer
...). This all leads to processor being "idle" (unused). To use processor at
all time the execution of such program was halted. At that point, a second (or
nth) program was started or restarted. User perceived that programs run "at the
same time" (hence the term, concurrent).

Shortly thereafter, the notion of a 'program' was expanded to the notion of an
'executing program and its context'. The concept of a process was born.

This became necessary with the invention of re-entrant code.  Threads came
somewhat later. However, with the advent of time-sharing; computer networks;
multiple-CPU, shared memory computers; etc., the old "multiprogramming" gave
way to true multitasking, multiprocessing and, later, multithreading.

urllib2
=======

functions
---------
* urlopen
* install_opener
* build_opener
* request_host
* _parse_proxy
* randombytes
* parse_keqv_list
* parse_http_list


class
-----
* Request
* OpenerDirector
* BaseHandler
  * HTTPErrorProcessor
  * HTTPCookieProcessor
  * HTTPDefaultErrorHandler
  * HTTPRedirectHandler
  * ProxyHandler
  * AbstractHTTPHandler
  * UnknownHandler
  * FileHandler
  * FTPHandler
  * CacheFTPHandler

* AbstractHTTPHandler
  * HTTPHandler
  * HTTPSHandler

* HTTPPasswordMgr
  * HTTPPasswordMgrWithDefaultRealm

* AbstractBasicAuthHandler

* AbstractBasicAuthHandler, BaseHandler
  * HTTPBasicAuthHandler
  * ProxyBasicAuthHandler

* AbstractDigestAuthHandler

* BaseHandler, AbstractDigestAuthHandler
  * HTTPDigestAuthHandler
  * ProxyDigestAuthHandler


urlopen -> build_opener -> OpenerDirector() -> OpenerDirector.add_handler for
each class and handler -> OpenerDirector.open() method on the composite object.
-> Request -> returns stateful url -> protocol_request is called -> _open -> and
protocol_response is called and returned.

the handlers must work in an specific order, the order is specified in a Handler attribute

/var/www/.htaccess is the file I am going to create with senthil:senthil

no_proxy

Some clients support the no_proxy environment variable that specifies a set of
domains for which the proxy should not be consulted; the contents is a
comma-separated list of domain names, with an optional :port part:

Python HTTP Authentication Schemes:
http://frontier.userland.com/stories/storyReader$2159


Call Stack:

Is a dynamic data structure that stores information about the active
subroutines.  Execution Stack, Control Stack, Runtime stack, function stack or
simply just a stack.  A call stack is used to keep track of the point to which
the sub-routine should return after finished executing.

Now you can compare the headers for expiry in cache control.

Header field definition:
http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html

* With that declaration, all characters in the source file will be treated as
  having the encoding *encoding*, and it will be possible to directly write
  Unicode string literals in the selected encoding.

* The list of possible encodings can be found in the Python Library Reference,
  in the section on 
  [http://docs.python.org/library/codecs.html#module-codecs codecs]

* By using UTF-8, most languages in the world can be used simultaneously in
  string literals and the comments.

* More examples:

   german_ae = unicode('\xc3\xa4','utf8')

::
        >>> german_ae = unicode("\xc3\xa4",'utf8')
        >>> sentence = "this is a " + german_ae
        >>> sentece2 = "Easy!"
        >>> sentence2 = "Easy!"
        >>> para = ".".join([sentence, sentence2])
        >>> para
        u'this is a \xe4.Easy!'
        >>> print para
        this is a ä.Easy!
        >>> 

* Without an encoding, the bytestring is essentially meaningless. 
* The default encoding assumed by Python is ASCII
* In order to convert unicode string back to bytestring.

::

  >>> bytestring = german_ae.decode('utf8')
  >>> bytestring
  \xc3\xa4
  >>> bytestring = german_ae.decode('latin1')
  >>> bytestring
  \xe4

BTW, both are same german characters albeit in different encodings.

What is the difference between process and a thread?

Both threads and processes are methods of parallelizing an application.
However, processes are independent execution units that contain their own state
information, use their own address spaces, and only interact with each other
via interprocess communication mechanisms (generally managed by the operating
system). Applications are typically divided into processes during the design
phase, and a master process explicitly spawns sub-processes when it makes sense
to logically separate significant application functionality. Processes, in
other words, are an architectural construct.

By contrast, a thread is a coding construct that doesn't affect the
architecture of an application. A single process might contains multiple
threads; all threads within a process share the same state and same memory
space, and can communicate with each other directly, because they share the
same variables.

Threads typically are spawned for a short-term benefit that is usually
visualized as a serial task, but which doesn't have to be performed in a linear
manner (such as performing a complex mathematical computation using
parallelism, or initializing a large matrix), and then are absorbed when no
longer required. The scope of a thread is within a specific code module—which
is why we can bolt-on threading without affecting the broader application.


4.21   How do you specify and enforce an interface spec in Python?

An interface specification for a module as provided by languages such as C++
and Java describes the prototypes for the methods and functions of the module.
Many feel that compile-time enforcement of interface specifications helps in
the construction of large programs.

Python 2.6 adds an abc module that lets you define Abstract Base Classes (ABC).
You can then use isinstance() and issubclass to check whether an instance or a
class implements a particular ABC. The collections modules defines a set of
useful ABC s such as Iterable, Container, and Mutablemapping.

For Python, many of the advantages of interface specifications can be obtained
by an appropriate test discipline for components. There is also a tool,
PyChecker, which can be used to find problems due to subclassing.

Handling Signals:

One of few things in Unix that do not confirm to file descriptors are
asynchronous events (signals); signals are received in signal handlers, small,
limited piece of code that run while rest of the task is suspended. 

Links
-----
Watch the Python Code Swarm here:
http://www.vimeo.com/1093745

For a better explaination on what is decorator:
http://personalpages.tds.net/~kent37/kk/00001.html

Alex-Martelli explaining Decorator:
http://code.activestate.com/recipes/52304/

Decorators: 
http://personalpages.tds.net/~kent37/kk/00001.html

http://www.rexx.com/~dkuhlman/python_book_01.html

Alex Martellis Callback tutorial: 
http://www.youtube.com/watch?v=LCZRJStwkKM
http://www.aleax.it/python_mat_en.html
http://www.strakt.com/dev_talks.html

http://code.activestate.com/recipes/52304/
http://stackoverflow.com/questions/136097/what-is-the-difference-between-staticmethod-and-classmethod-in-python
http://personalpages.tds.net/~kent37/kk/00001.html
http://jessenoller.com/2009/02/01/python-threads-and-the-global-interpreter-lock/

The Evolution of  Python Programmer
http://gist.github.com/289467

[http://code.activestate.com/recipes/86900/ Factory Example]
[http://www.suttoncourtenay.org.uk/duncan/accu/pythonpatterns.html Python Patterns]


Norman Matloff's Python Tutorials
---------------------------------

1) http://heather.cs.ucdavis.edu/~matloff/python.html 

Python and Vim
--------------
http://henry.precheur.org/2008/4/18/Indenting_Python_with_VIM.html
http://blog.sontek.net/2008/05/11/python-with-a-modular-ide-vim/ 
